# Prompt-Tuning Playground

ðŸ§ª A hands-on tool for experimenting with and comparing large language model (LLM) prompts.

## ðŸš€ Features
- Compare multiple prompts side-by-side
- Inject user input into prompt templates
- View LLM responses with ratings and comments
- Save and load history of prompt runs for later review

## ðŸ“¦ Tech Stack
- Python
- [Streamlit](https://streamlit.io)
- OpenAI API (or other LLM endpoints)
- Local JSON for lightweight storage

## ðŸ“‚ Project Structure
```
prompt-tuning-playground/
â”œâ”€â”€ app.py                   # Streamlit front-end
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ prompt_runner.py    # LLM API interface and prompt handling
â”‚   â””â”€â”€ history.py          # History load/save functionality
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ history.json        # Stored prompt evaluations
```

## ðŸ”§ Setup
1. Clone the repo:
```bash
git clone https://github.com/YOUR_USERNAME/prompt-tuning-playground.git
cd prompt-tuning-playground
```
2. Install dependencies:
```bash
pip install -r requirements.txt
```
3. Run the app:
```bash
streamlit run app.py
```

## ðŸ›  Requirements
- Python 3.8+
- OpenAI API key (set as `OPENAI_API_KEY` env variable)

## ðŸ“˜ License
MIT

## ðŸ“„ requirements.txt
```
streamlit
openai
python-dotenv
```
*You may need to add additional dependencies later, like `langchain` or `faiss` if you expand the project.*

